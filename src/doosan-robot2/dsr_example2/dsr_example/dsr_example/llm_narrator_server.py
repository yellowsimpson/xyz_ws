#!/usr/bin/env python3
"""
ë¬´ë£Œ LLM(Ollama) ê¸°ë°˜ ë‚´ë ˆì´ì…˜ ì„œë²„ (FastAPI ë²„ì „)

- POST /narrate
  body: {"event": "xy_aligning", "state": {...}}
  return: {"text": "...ë¡œë´‡ ì„¤ëª… ë©˜íŠ¸..."}

Ollama:
  - localhost:11434
  - model: llama3.2:3b (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ)
"""

from fastapi import FastAPI
from pydantic import BaseModel
import requests
import json

# --------- ìš”ì²­ ë°”ë”” ìŠ¤í‚¤ë§ˆ ---------
class NarrateRequest(BaseModel):
    event: str
    state: dict | None = None

# --------- FastAPI ì•± ---------
app = FastAPI()

@app.post("/narrate")
async def narrate(req: NarrateRequest):
    """
    ë¡œë´‡ ìƒíƒœ(event)ë¥¼ ë°›ì•„ì„œ
    Ollama(llama3.2:3b)ì—ê²Œ ë‚´ë ˆì´ì…˜ ë¬¸ì¥ì„ ìƒì„±ì‹œí‚¤ëŠ” ì—”ë“œí¬ì¸íŠ¸
    """
    # 1) í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°„ë‹¨ ë²„ì „)
    prompt = f"""
ë„ˆëŠ” ì£¼ìœ  ë¡œë´‡ì˜ ë‚´ë ˆì´ì…˜ AIë‹¤.
í˜„ì¬ ë¡œë´‡ì˜ ë‹¨ê³„ëŠ” '{req.event}' ì´ë‹¤.
ì‚¬ìš©ìì—ê²Œ ìƒí™©ì„ ì¹œì ˆí•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ì„¤ëª…í•´ë¼.
ë°˜ë§ ë§ê³  ì¡´ëŒ“ë§ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ.
"""

    try:
        # 2) Ollamaì— ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
        resp = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:3b",  # ğŸ‘‰ ì—¬ê¸° ì´ë¦„ì´ ollama list ì— ìˆëŠ” ì´ë¦„ê³¼ ê°™ì•„ì•¼ í•¨
                "prompt": prompt,
                "stream": True
            },
            stream=True,
            timeout=30,
        )

        full_text = ""

        # 3) ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì˜¤ëŠ” JSON ë¼ì¸ë“¤ì„ í•˜ë‚˜ì”© ì´ì–´ë¶™ì´ê¸°
        for line in resp.iter_lines():
            if not line:
                continue
            try:
                obj = json.loads(line.decode("utf-8"))
            except Exception:
                continue

            chunk = obj.get("response", "")
            full_text += chunk

            # done í”Œë˜ê·¸ ìˆìœ¼ë©´ ì¢…ë£Œ
            if obj.get("done"):
                break

        full_text = full_text.strip()
        print("[LLM RAW TEXT]", full_text)

        # 4) ë¹„ì–´ ìˆìœ¼ë©´ fallback ë©˜íŠ¸
        if not full_text:
            return {"text": "ì•ˆë‚´ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        # 5) ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜
        return {"text": full_text}

    except Exception as e:
        print("[LLM ERROR]", e)
        return {"text": f"LLM ì„œë²„ ì˜¤ë¥˜: {e}"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
